# 基于信息熵改进的古籍图像文字聚类算法

提出一种基于信息熵评估聚类中心特征的标准，其中类簇中心被具有较低局部密度的邻居点包围，且与具有更高密度的任何点有相对较大的距离。

# 前言
## 传统聚类算法
- 需要一些参数限制
- 需要设定聚类的数目
- 需要设置很多的参数
- 聚类初始形态及参数非常敏感
- 计算时间长


## Clustering by fast search and find of density peaks
- 聚类中心的密度高于周围邻居密度(类簇中心被具有较低局部密度的邻居点包围)
  - 存在中心点集合： $\{ i\mid i\subset I\}$
  - 定义点i的局部密度为 $\rho_i$

- 与具有更高密度的任何点有相对较大的距离
  - 存在中心点集合： $\{ i \mid i\subset I\}$
  - 定义点i到某点局部高密度为 $\delta_i$
  - i点到j点的两点距离 $d_{ij}$

# 国内研究现状
- 基于群体智能的聚类算法
  - 1999, From Natural to Artificial System [M](群体智能)
  - 1991, The dynamics of collective sorting robot-like ants and ant-like robots[J]
  -
#主要步骤
## 密度聚类
## 图像集初始化
//TODO
### 求 $\rho_i$
局部密度公式：
 $$\rho_i=\sum_j\chi\left(d_{ij}-d_c\right)$$
 其中，如果x<0，那么 $\chi\left(x\right)=1$ ；否则，是一个截断距离。基本上， $d_c$ 等于与点的距离小于 $d_c$ 的点的个数。算法只对不同点的 $\rho_i$ 的相对大小敏感，这意味着对于大数据集，分析结果对于 $d_c$ 的选择有很好鲁棒性。

 > $d_c$类似指定了最小的聚类的类密度半径，类内距离
### 求 $\delta_i$
定义点i到某点j的局部密度公式：
$$\delta_i=min_{j:\rho_j>\rho_i}(d_{ij})$$

> $\rho_j>\rho_i$说明类密度中，谁的密度更密集，控制调节类的合并,合并到一个类簇，类间距离

### 求基于 $\rho_i$ 与 $\delta_i$ 的决策图

![决策图](http://img.blog.csdn.net/20141021210152812)
>图A描述的存在二维空间数据集合28个点，根据所求得的 $\rho_i$ 与 $\delta_i$ 的决策树，人工决策聚类的个数

![样列](http://img.blog.csdn.net/20141021213017742)

>图2.合成点分布的结果。

>(A)绘制的点分布的概率分布。(B和C)分别为4000和1000样本点的点分布。每个点以其颜色表示所属类簇，黑色点属于光晕类簇。(D和E) 相应的决策图，彩色的点表示类簇中心。(F)被归属到错误的类簇的点的比例作为样本维度的函数。误差线表明均值的标准差。

>为图2.B中数据赋予不同的值，却得到几乎一样的结果。一般来说，我们可以选择使得点的平均邻居数大概是数据集中点的总数的1-2%。对于较小的数据集，可能会被大的统计误差影响，在这种情况下，需要通过更准确的方法估计密度(例如可以采取文章中提到的指数核的方法)。
>


#### 问题
无法解决聚类个数,即对于 $d_c$ 无法处理很好

## 信息熵改进
### 信息熵定义
假设 $x$ 是一个随机变量，$X$是其可能的取值集合（连续型数据需离散化）, $p(x)$ 是取 $x$ 值的可能性函数，信息熵 $E(x)$ 定义为公式如下：
$$E(x)=-\sum_{x \in X}{x}log{p(x)}$$
$$\Updownarrow$$
$$ H=-\sum_{i=1}^n\frac{c_1}{N}log(\frac{c_i}{N}) $$

通过结合Clustering by fast search and find of density peaks，定义 $H$ 为熵值，$c_i$ 代表第 $I$ 个聚类的成员数，$N$ 代表总共的聚类数，可以给出式(3)：

# 备注
## 信息熵
一个多变量向量 $x=\{x_1,x_2,....,x_n\}$ 的信息熵按单变量信息熵公式计算，其中: $p(x)=p(x_1,x_2,...,x_n)$ 是多变量可能分布函数，$X_1,X_2,...,X_n$ 是相应向量项的可能取值集合（连续型数据需离散化）
$$E(x)=-\sum_{x_1 \in X_1}....\sum_{x_n \in X_n}{p(x_1,x_2,...,x_n)logp(x_1,x_2,
  ...,x_n)}$$
l

## 算法

聚类方法处理图像标注问题，主要承担的是对未知特征的提取，并组合成一组具有相似的集合，通过聚类算法，我们能够将未标注的样本在一定程度上区分开。首先假设：类簇中心被具有较低局部密度的邻居点包围，且与具有更高密度的任何点有相对较大的距离。简单的理解规则就是类内间距小，类间间距大。对于每一个数据点，通过计算两个量：点的局部密度 $\rho_{i}$ 和该点到具有更高局部密度的点的距离 $\delta_{i}$ ，而这两个值都取决于数据点间的距离 $d_{ij}$ ，对于水书文字图像间的 $d_{ij}$ 主要通过马氏距离等距离度量方法进行求解。
数据点i的局部密度 $\rho_i$ 定义为式(1)：

 $$\rho_i=\sum_j\chi\left(d_{ij}-d_c\right)$$

其中，如果x<0，那么 $\chi\left(x\right)=1$ ；否则，是一个截断距离。基本上， $d_c$ 等于与点的距离小于 $d_c$ 的点的个数。算法只对不同点的 $\rho_i$ 的相对大小敏感，这意味着对于大数据集，分析结果对于 $d_c$ 的选择有很好鲁棒性。数据点i的是点 $\delta_i$ 到任何比其密度大的点的距离的最小值式(2)：
$$\delta_i=min_{j:p_j>p_i}(d_{ij})$$
对于密度最大的点，我们可以得到 $\delta_i=max_j(d_{ij})$ 。当类簇中心找到后，剩余的每个点被归属到它的有更高密度的最近邻所属类簇。类簇分配只需一步即可完成，不像其它算法要对目标函数进行迭代优化。在聚类分析中，定量的衡量分配的可信度是很重要的。在该算法中，首先为每个类簇定义一个边界区域(即分配到该类簇但于其它类簇的点的距离小于的点的集合)，然后为每个类簇的找到其边界区域中密度最高的点，并以来表示该点的密度。类簇中局部密度值比大的点被看作是类簇的核心部分(即分配到该类簇的可靠性较高)，其他点被看作是类簇的光晕部分或者称作噪声。

通过引入信息熵优化的方法与结合大数定理理论，定义H为熵值，Ci代表第I个聚类的成员数，N代表总共的聚类数，可以给出式(3)：

$$ H=-\sum_{i=1}^n\frac{c_1}{N}log(\frac{c_i}{N}) $$

在聚类的初始时刻，假设系统模型下的每个成员都为单独的类即`$c_i$`取得最大值，那么模型下的信息量就达到最大，即系统的信息熵H为最大值，随着每次聚类规则的变化，系统的信息熵也会紧密的相关变化，通过观察与分析熵的梯度变化，通过了解整个系统的模型下熵值变化情况，推导出在单位时间片内熵值下降稳定情况。当发生多个时间片内未发送熵值变化，即达到系统模型下最为优异的聚类个数，从而完成对数据的聚类划分工作，进而实现对给定的集合加上标签。
